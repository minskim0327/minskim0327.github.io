---
layout: post
title: 컴퓨터 구조2 (feat. KU.컴구2)
category: [computer_architecture, cpu, parallel_programming]
author: hyungsun
image: assets/images/comarch2_1.PNG
---
# 1. Why Paralell Computing?
성장에 한계점이 온 CPU로는 복잡한 컴퓨팅을 필요로 하는 빅데이터 분석이나 기후 과학 등의 분야를 성능적인 측면에서 만족시키지 못했다.
따라서 CPU를 병렬적으로 프로그래밍하여 성능 향상을 꾀해 이를 극복하는 방법을 알아야 한다.

### 병렬화의 예시
단순하게 아래와 같은 프로그램을 병렬화 시키는 과정을 생각해보자.
```C
sum = 0;
for (i=0;i<n;i++) {
    x = Compute_next_value(...);
    sum += x;
}
```

만약 우리가 `n`보다 훨씬 작은 수의 `p`개의 코어를 가지고 있다면, 각 코어는 아래와 같이 `n/p` 개의 부분합을 구할 것이다. 그리고 이 부분합들은 `master core`에서 합쳐지게 된다.
```C
if (I'm the master core) {  // 마스터 코어의 코드
    sum = my_x;
    for each core other than myself {
        receive value from core;
        sum += value;
    }
}else { // 부분합을 구하는 코어의 코드
    my_sum = 0;
    my_first_i = ...;
    my_last_i = ...;

    for (my_i=my_first_i;my_i<my_last_i;my_i++) {
        my_x = Compute_next_value(...);
        my_sum += my_x;
    }
    send my_x to the master;
}
```

하지만 이러한 방식보다 더 좋은 `global sum`을 구하는 방식이 있다. 다음과 같이 생각해보자.
- 코어를 2개씩 묶어 `core 2n`의 결과를 `core 2n+1`의 결과와 합치게 하여 부분합을 혼자 합쳐야 하는 `master core`의 부담을 줄인다.
- 위 과정이 끝난 후에는 `core 4n`와 `core 4n+2`에 대해 같은 작업을 한다.
- 마찬가지로 결국 코어의 합이 `core 0`에 모두 쌓일 때까지 진행한다.

그림으로 표현하면 아래와 같다. (`p` = 8)
![병렬 프로그래밍(Better Algorithm)]({{ site.url }}/assets/images/comarch2_1.PNG)

### 왜 아래의 방식이 더 나은가?
#### 첫 번째 방식
`master core`는 다른 코어로부터 `my_sum`을 7번 받아 7번의 합을 수행했다(비용 14).
#### 두 번째 방식
위 그림에서 0번 core가 바로 `master core`인데, 이 코어는 다른 코어로부터 `my_sum`을 3번 받아 3번의 합을 수행했다(비용 6).  

결국 2배 정도 성능이 향상된 것이다.

만약 코어가 1000개라면 성능이 더욱 극적으로 향상 된다.  
첫번째 방식에서는 999 + 999 = 1998 의 비용이 들고,  
두번째 방식으로는 10 + 10 = 20 의 비용이 들기 때문이다. (성능 향상 100배)

### Coordination
각 코어들은 작업을 협력적으로 수행(이를 Coordination이라 한다)해야 하는데, 다음을 주의해야 한다.
- Communication : 통신, 프로세스 간 데이터를 전달할 수 있어야 한다.
- Load balancing : 부하 분산, 한 프로세스만 고역에 시달리게 두어선 안 된다.
- Synchronization : 동기화, 한 프로세스의 작업이 다른 프로세스들 보다 지나치게 앞서 있어선 안된다.

### 병렬 시스템
병렬 시스템에는 2가지 타입이 있다.  
- Shared-memory(a) : 각 코어가 컴퓨터의 메모리를 공유한다. 공유 메모리를 검증하고 업데이트 하여 Coordinate 하는 방식이다.  
- Distributed-memory(b) : 각 코어는 자신의 독자적인 메모리가 있다. 네트워크를 통해 messages를 전달하여 Coordinate 한다.
![병렬 프로그래밍(타입)]({{ site.url }}/assets/images/comarch2_2.PNG)

# 2. Parallel Hardware and Parallel Software #1
### 배경지식
폰 노이만 아키텍쳐(Von Neumann Architecture)는 다음과 같은 구조를 가진다.
![Von Neumann Architecture]({{ site.url }}/assets/images/comarch2_3.PNG)
ALU가 Main Memory의 Address에 접근하여 가져온 Contents를 Control Unit에서 제어한다는 의미이고, 간단하게 ALU는 노동자, CPU는 보스에 대응된다고 보면 된다.
메인 메모리 같은 경우 주소들의 집합이며 instruction과 data 모두를 포함하고 있다.

#### 용어 
- Register: CPU의 일부로 속도가 빠른 저장 공간.
- Program Counter: 다음에 시작될 Insturction의 주소를 저장.
- Bus: CPU와 memory를 연결해주는 선.

### Process란?
컴퓨터 프로그램이 실행되는 인스턴스를 의미한다. 구성요소는 다음과 같다.
- 실행가능한 기계어 프로그램
- 메모리 블록(Stack, heaps, etc)
- 프로세스가 할당된 OS resources의 Descriptor
- 프로세스 상태 정보

### Multitasking and Threading
- Multitasking: 싱글 프로세스에게 여러개의 프로세스가 돌아가는 것처럼 착각을 하게 만든다. 이 때 연산을 위해 CPU를 점유하는데 time slice(시분할)방식으로 잠깐 잠깐씩 점유를 한다. 잠깐 점유를 하고 있는 동안 작업이 다 끝나지 못하는 경우 block당하여 다른 프로세스가 끝나길 기다려야 한다.
- Threading: 쓰레드는 우선 프로세스에 포함되는 개념이다. 쓰레드는 각자 독립적인 작업이 가능하다. 만약 한 스레드가 지연등의 이유로 블록당한 상태라면 다른 스레드가 스위칭 프로세스를 스위칭하는 것보다 스레드를 스위칭하는 것이 훨씬 빠르다. 
  
### 캐싱의 특징
- Spatial Locality : 한 번 접근한 주소의 근처에 다시 접근할 가능성이 높다.
- Temporal Locality : 한 번 접근한 주소는 다시 접근할 가능성이 높다

### 폰 노이만 아키택쳐 개선
우리는 성능향상을 위해 캐시를 사용하지만, CPU가 캐시에 Data를 쓰게 되면 캐시는 메인 메모리에 있는 데이터와 일치하지 않게 된다.
그래서 2가지 방식으로 이를 해결한다.
- Write-through: 캐시에 데이터가 띄여질 때, 메인 메모리까지 쭉 올라가면서 업데이트를 시켜준다.
- Write-back: 해당 데이터에 dirty bit 마크를 해놓고, 그 캐시가 새로운 캐시로 교체될 때 dirty bit를 메모리에 쓴다.

#### Cache Maping
캐시 매핑을 개선하여 성능을 향상시킬 수 있는데, 3가지 방식이 있다.
- Full Associative
- Direct Mapped
- N-way set Associative

### Super Scalar
컴파일 시에 쓰레드 스케쥴링을 하는 방식을 Static Multiple issue라 하고, 
run-time에 쓰레드 스케쥴링을 하는 방식을 Dynamic Multiple issue라 하는데 이 방식을 지원하는 하드웨어를 Super Scalar라고 한다.

### Hardware Multithreading
Static 방식
- Fine-grained: stall 없이 계속 각 명령어를 실행하다가 stall을 만나면 무조건 스위칭하는 방식
- Coarse-grained: Fine-grained보다 작업단위를 더 크게 하여 너무 자주 스위칭 하지 않도록 하는 방식.  

Dynamic 방식
- Simultaneous multithreading(SMT): stall을 신경쓰지 않고, run-time에 지정해준 작업단위로 처리하는 방식

# 3. Parallel Hardware and Parallel Software #2

### 병렬 시스템, 플린의 분류
- SISD: 전통적인 폰 노이만 아키택쳐에서 사용하는 방식, 싱글 instruction 스트림, 싱글 데이터 스트림
- SIMD: 싱글 instruction 스트림, 멀티 데이터 스트림
- MISD: 다루지 않음.
- MIMD: 멀티 instruction 스트림, 멀티 데이터 스트림

### SIMD
Data Parallelism 이라고도 한다.
CPU에 ALU가 여러개 달려 있는 형태로, 여러 ALU에서 데이터를 병렬적으로 접근할 수 있게 해주는 방식이다.
가령 아래와 같은 코드가 있다고 하자. 
```C
for (i=0;i<n;i++) {
    x[i] += y[i];
}
```

`n`이 15 이고 ALU의 개수가 4개라면 SIMD에서는 아래와 같이 라운드 마다 `x`에 병렬적으로 접근하게 된다.
![SIMD]({{ site.url }}/assets/images/comarch2_4.PNG)

SIMD가 사용되는 대표적인 예로 Vector Processors와 GPU가 있다.

#### Vector Processors
기존 CPU가 개별 데이터 요소 또는 스칼라를 처리하는 방식인 반면 Vector Processor는 배열 또는 벡터 데이터를 처리하는 방식으로 작동한다.
- 벡터 레지스터.  
피연산자의 벡터를 저장하고 그 내용에 동시에 작업 할 수 있다.
- 벡터화되고 파이프 라인 된 기능 단위.  
동일한 작업이 벡터의 각 요소 (또는 요소 쌍)에 적용된다.
- 벡터 지침.  
스칼라가 아닌 벡터에서 작동한다.
- interleaved 메모리.  
메모리의 다중 "뱅크 (banks)"는 다소 독립적으로 접근 할 수 있다.
벡터의 요소를 여러 뱅크에 분산 시키므로 연속 요소들의 Load / Stroring 지연을 줄이거나 없앨 수 있다.
- 스트라이드 된 메모리 액세스 및 하드웨어 분산 / 수집  
프로그램은 고정 된 간격으로 배치 된 벡터 요소에 접근한다.

이점
- 빠르다.
- 사용하기 쉽다.
- 벡터화 컴파일러는 악용 코드를 식별하는 데 유용하다.  
- 컴파일러는 벡터화 할 수 없는 코드에 대한 정보도 제공 할 수 있다.
- 프로그래머가 코드를 재평가 할 수 있도록 도와준다.
- 높은 메모리 대역폭.
- 캐시 라인의 모든 항목을 사용한다.  

단점
- 그들은 다른 병렬 아키텍처뿐만 아니라 불규칙한 데이터 구조도 처리하지 않는다.
- 더 큰 문제를 처리 할 수 있는 능력에 대한 제한이 있다. (확장성)

#### GPU
그래픽 처리 파이프 라인은 내부 표현을 컴퓨터 화면으로 보낼 수있는 픽셀 배열로 변환한다.
이 파이프 라인의 여러 단계 (셰이더 함수라고 함)는 프로그래밍 가능하다.

셰이더 함수는 병렬적으로 처리되며, 그래픽 스트림 안의 여러 요소에 진입할 수 있다.
GPU는 SIMD를 이용해서 성능을 최적화 한다.
GPU는 상대적으로 작은 문제에 대해서 더 안좋은 성능을 내게 된다.

### MIMD
MIMD 시스템은 보통 비동기식 방식으로 구성된다. 
이는 여러 개의 코어가 각자 자신들의 ALU를 가지고 있는 형태이다.
MIMD에는 Shared Memory System과 Distributed Memory System 이렇게 2가지가 있다.

#### Shared Memory System
프로세서들은 interconnection network를 통해 메모리 시스템에 연결된다.
이 때 각 프로세서들은 각자 따로 메모리 주소에 접근이 가능하다.
프로세서는 일반적으로 Shared Memory 구조에 액세스하여 데이터를 교환한다.
가장 널리 사용되는  Shared Memory 시스템은 하나 이상의 멀티 코어 프로세서를 사용한다.
(단일 칩에 여러 개의 CPU 또는 코어가 있음)
Shared Memory System에는 Normal, UMA, NUMA 이렇게 3가지 종류가 있다.
1. Normal  
가장 단순한 shared memory System 구조이다.
![Normal SMS]({{ site.url }}/assets/images/comarch2_5.PNG)

2. UMA Multicode System  
모든 코어는 같은 시간에 Memory에 접근하게 된다.
![UMA]({{ site.url }}/assets/images/comarch2_6.PNG)

3. NUMA Multicode System  
NUMA는 UMA구조보다 메모리가 더 클 경우에 각 칩들마다 사용가능한 메모리 구간을 나눠놓은 것이다.
이 코어가 직접 연결되어있는 메모리 위치는 다른 칩을 통해 액세스해야하는 메모리 위치보다 빠르게 액세스 할 수 있다.
![NUMA]({{ site.url }}/assets/images/comarch2_7.PNG)



#### Distributed Memory System
분산 메모리 시스템에서 각 프로세서는 자체 메모리를 가지고 있으며 `프로세서-메모리` 쌍은 interconnection network를 통해 통신한다.
통신은 프로세서 쌍들간에 메시지를 송수신함으로써 명시적으로 구현된다.
![DMS]({{ site.url }}/assets/images/comarch2_8.PNG)

- Clusters : `프로세서-메모리` 쌍들을 Clusters라 하고 이들은 모두 범용 interconnection network로 연결된다.
즉 클러스터의 노드는 각각 통신 네트워크에 의해 결합된 독립적인 컴퓨터이다.

## Interconnection Network
아까 부터 계속 등장해 눈살을 찌뿌리게 했던 녀석인데 코어 간 데이터를 전달할 수 있도록 다리 역할을 한다. 여기에는 2가지 종류가 있다.
- Shared Memory Interconnects
- Distributed Memory Interconnects

### Shared Memory Interconnects
Bus, Switched 2가지 방식이 있다.
- **Bus Interconnect**  
비용이 싸다.  
버스에 연결된 장치의 수가 증가하면 성능이 저하된다.  
- **Switched Interconnect (Cross bar)**  
비용이 많이 든다.  
서로 다른 장치 간에 동시적인 통신이 가능하다.
아래는 크로스 바에 관한 그림이다. 
![Switched Interconnect]({{ site.url }}/assets/images/comarch2_9.PNG)
크로스 바의 구조가 (a)와 같을 때, (b)처럼 연결을 제어하여 결국은 (c)처럼 동시적으로 통신한다는 의미이다.

### Distributed Memory Interconnects
분산 메모리는 기본적으로 프로세스끼리 메모리를 따로 가지고 있다는 것을 의미한다.
눈여겨 보아야 할 것은 이런 상황에서 다른 프로세스가 가지고 있는 메모리에 어떻게 해야 효율적으로 접근할 수 있나? 이다.

여기에는 Direct, Indirect 2가지 방식이 있다.

#### Direct interconnect
Direct interconnect 모델은 스위치가 `프로세서-메모리`쌍에 직접 연결되어 있고, 스위치들 간에도 상호 연결된 구조이다.
그 종류로는 `Ring`, `Toroidal mesh`, `Fully Connected`, `Hypercude` 이렇게 4가지 종류가 있다.
![Direct interconnect]({{ site.url }}/assets/images/comarch2_10.PNG)
(a)는 `ring` 형이며, (b)는 `toroidal mesh` 형이라고 한다. toroidal mesh는 (a)에서 차원을 확장 시켜놓은 형태로 양 극점까지 전부 다 연결해 놓은 구조를 의미한다.
이 때 **Worst case**에 얼마나 많은 커뮤니케이션이 동시에 발생할 수 있는 가를 `Bisection Width`로 나타내는데, `Bisection Width`이 클수록 더 좋은 성능을 낸다.  

예를 들어 아래의 **Ring** 구조의 경우를 살펴보자.
![Ring]({{ site.url }}/assets/images/comarch2_11.PNG)
A와 B는 서로 다른 Device를 의미한다. (a)는 best case에 해당하고 (b)는 worst case에 해당한다. `Bisection Width`는 worst case에 해당하고 이 경우 `A-B` 쌍의 개수는 고작 2개이다. 따라서 위 그림과 같은 구조에서 `Bisection Width`는 2 이다.  

또, **toroidal mesh**도 같은 방식으로 `Bisection Width`를 구할 수 있다.
![toroidal mesh]({{ site.url }}/assets/images/comarch2_12.PNG)
worst case는 `toroidal mesh`를 정 중앙으로 자르는 직선과 만나는 점의 개수이다. 즉 `Bisection Width`는 8 이다. 프로세서 개수를 \\({p}\\) 라고 할 때 toroidal mesh 에서는 `Bisection Width`가 \\(\frac{p}{2}\\) 라고 생각하면 편하다.  

**Fully Connected Network** 모델을 생각해보자. 구조는 다음과 같다.
![Fully Connected Network]({{ site.url }}/assets/images/comarch2_13.PNG)
이 그림에서 `Bisection Width`는 얼마나 될까? 직접 세어보면 알겠지만 9개이다. `Fully Connected`에서는 \\({\frac{p^{2}}{4}}\\)으로 `Bisection Width`를 구할 수 있다.
하지만 이 모델은 연결되는 링크 수 에 비해 `Bisection Width`가 적어서 실용적이지 못하다.  

다음으로 **Hyper Cube**모델을 살펴보자. 구조는 다음과 같다.
![Hyper Cube]({{ site.url }}/assets/images/comarch2_14.PNG)
(a) > (b) > (c) 순서로 차원이 높아지는 구조이다. 차원이 높아졌다는 것은 곧 한 노드에서 다른 노드로 나가는 링크의 개수가 증가했다는 뜻이다. 만약 n차원 만큼 Hypercude를 확장시켰다고 했을 때 `Bisection Width`는  \\(\frac{p}{2}\\) 로 계산되어 진다.

#### Indirect interconnect
모든 스위치가 프로세서가 직접 연결되지는 않은 구조이다.
`Crossbar` 와 `Omega Network` 이렇게 2종류가 여기에 해당 된다.  

**Crossbar**부터 살펴보자. `crossbar`는 언뜻 보면 전부 이어져 있는 것 같지만 사실은 그렇지 않다. 각 노드가 프로세서에 해당되는 것이 아니라 그냥 데이터의 흐름을 컨트롤 해주는 녀석들이기 때문이다. 따라서 `Crossbar`와 프로세서를 같이 나타내면 다음 그림과 같다.
![Crossbar]({{ site.url }}/assets/images/comarch2_15.PNG)
굳이 분석해 보지 않아도 엄청나게 비효율적인 모델이라는 것은 명백해 보인다.  프로세서 개수를 \\({p}\\) 라고 할 때 크로스바를 구현하기 위해 필요한 스위치의 개수는 \\({p}^{2}\\) 이다.

이 `Crossbar`의 단점을 보완하고자 나온 모델이 바로 `Omega Network`이다. `Butterfly Switch`라는 예쁜 이름으로도 불린다. 어디가 나비 같은지 모르겠는데 아무튼 닮았다고 한다. 구조는 아래 그림과 같다.
![Omega Network]({{ site.url }}/assets/images/comarch2_16.PNG)
그림에서 보여주듯이 왼쪽 그림의 동그라미가 오른쪽 그림(switch)에 해당한다. 스위치의 개수가 크로스바보다 줄어들게 되는데, 오메가 네트워크를 구현하기 위해 필요한 스위치의 개수는 \\(2p\log{_2}{p}\\)로 나타내어 진다. 결국 오메가 네트워크는 크로스바보다 비용이 훨씬 더 적게 드는 장점을 가지고 있는 모델이라고 할 수 있다.

### 성능 평가
성능 측정은 결국 시간 측정으로 귀결된다. 이건 하드웨어 성능평가 할 때 신물나게 해봤을 것이다. 
프로세서 네트워크에서는 서킷스위칭은 안하고 패킷스위칭은 한다. 서킷 스위칭은 링크를 점유하기 때문에 데이터가 전송하는 동안 다른 데이터가 들어갈 수가 없기 떄문이다. 이 때 네트워크상에서 날아다니는 패킷은 진짜 작은 바이트 정도의 크기이고 보통 Message라고 표현한다.
그렇다면 스위치 간 Message Transmission Time 은 어떻게 구할까? 이를 구하기 위해선 다음 2가지를 알아야 한다.
- Latency : 데이터 출발에서 받을 때까지 걸리는 시간(준비하고 보내는 시간)
- Bandwidth : 얼마만큼 한꺼번에 전송이 가능한가  
그냥 \\(l\\)이 Latency, \\(n\\)이 Message Length, \\(b\\)가 bandwidth 라고 하면 Message Transmission Time은 \\(l+\frac{n}{b}\\) 이다.
굳이 쉬운 걸 어렵게 표현했다는 생각이 들지만 넘어가기로 하자.

### 캐쉬 일관성, Cache Coherence
평범한 Shared Memory System 에서 우리는 아래와 같은 작업하려고 한다.
<!--![Cache Coherence]({{ site.url }}/assets/images/comarch2_17.PNG)-->
![Cache Coherence]({{ site.url }}/assets/images/comarch2_18.PNG)
우리는 현재 `x`라는 변수를 공유하고 있다고 가정하자.  
time 0에서 Core 0은 `x`를 읽어와서 캐시에 넣어둔 상황이다(사용했으므로). Core 1은 `3 * x`를 했으므로 `y1` 에 `6` 이 들어간다.  
time 1에서 Core 0은 `x`에 `7`을 넣었긴 하지만 Core 1 에서는 `x`가 아직 2 에서 업데이트 되지 않았다.  
time 2에서 Core 0은 `x`와 관련된 이벤트가 발생하지 않아 동기화를 시켜주지 않는다. Core 1에서는 그대로 `x`는 `2`이므로 `z1`에는 `8`이 들어간다.  

이것은 결국 Cache coherence를 잘 지켜내지 못한 경우이고 이를 막기 위해 2가지 방식을 사용한다.

- Snooping Cache Coherence : 캐시와 코어 사이에 메모리 버스를 두고 이벤트가 발생할 때마다 트렌젝션을 걸어 처리하게 만든다. 
- Directory Based Cache Coherence : 각 캐시 라인의 상태를 저장하는 directory라는 자료구조를 사용해 캐시 업데이트를 처리한다.

# 4. Parallel Software
앞으로 설명할 병렬 소프트웨어는 모두 MIMD를 기준으로 한다.  
병렬적으로 프로그래밍을 한다는 것은 각 쓰레드(프로세스)에 작업배분(load balacing)을 잘해야 한다는 뜻이다. 이 때 필연적으로 쓰레드(프로세스) 간에 데이터를 주고 받아야 하는 경우가 생기는데 이를 보고 communication이라 한다. communication은 오버헤드가 매우 크므로 최소화해야 한다.

기본적으로 병렬 프로그래밍은 `Nondeterminism` 하다. 순서가 정해져 있지 않다는 것인데, 아래 작업을 보자.
![Parallel Software]({{ site.url }}/assets/images/comarch2_19.PNG)
아까와 마찬가지고 변수 `x`를 공유하고 있는 상황이다.  
Time 4 에서 Core 0이 `x = 7`을 해주고 Time 5 에서 Core 1이 `x = 19`를 해주는데, 비록 Time 5에 `x = 19`를 먼저 넣어줬더라도 쓰레드의 진입시간은 정해져 있기 때문에 `x`에는 최종으로 `7`이라는 값이 들어갈 수도 있다. 이것이 바로 `Nondeterminism`이다.
  
이러한 `Nondeterminism`으로 인해 한 자원을 두고 서로 경쟁하는 `Race Condition`이 발생하는데, 이 상황에서 한 프로세스만 접근하게 할 수 있도록 `Critical Section`이라는 것을 만들어 냈다. 이는 특별한 하드웨어적 컨트롤이 있는 것이 아닌 프로세스 간의 약속이라고 생각해야 한다.

이 `Critical Section`을 이용해 병렬적으로 프로그래밍하는 방식은 Busy-Waiting, Message Passing, PGAS 이렇게 3가지가 있다.

### Busy-Waiting
가장 기본적인 Critical Section을 이용한 것이 Busy-Waiting이고 코드는 다음과 같다.
```C
my_val = Compute_val( my_rank ) ;
if ( my_rank == 1)
     while ( ! ok_for_1 );  /* ok 신호가 떨어질 때까지 기다린다 */
x += my_val;                /* Critical section, 쓰레드 1은 함부로 이 영역으로 들어올 수 없다. */
if ( my_rank == 0)
     ok_for_1 = true;       /* ok 신호를 보낸다 */
```
Busy-wait loop로 구현을 하게 되면 공유자원이 선점되어 있는 동안 다른 쓰레드가 접근하지 못한다. 공유자원을 써도 되냐고 물어보는 빈도가 매우 빠르기 때문에 오히려 더 비효율적일 수도 있다. 


### Message Passing
Message Passing에서는 프로세스를 sleep시켰다가 wake up 시그널을 보내 깨우는 것을 구현했다. 
시그널 통신방식을 사용하므로 Message Passing이라는 이름이 지어졌다.

```C
char message [100];
...
my_rank = Get_rank();   // 프로세스 id를 구한다.
if ( my_rank == 1 ) {
     sprintf ( message , "Greetings from process 1" ) ;
     Send ( message , MSG_CHAR , 100 , 0 ); // thread 0 에게 message를 보낸다.
} else if ( my_rank == 0 ) {
     Receive ( message , MSG_CHAR , 100 , 1 ); // thread 1 로부터 message를 받아 잠에서 깨어난다.
     printf ( "Process 0 > Received: %s\n" , message ) ;
}
```


### Partitioned Global Address Space (PGAS) Languages
요즘 에는 PGAS (피가스 ..라고 읽더라) 라는 걸 쓴다.
요즘 컴퓨터는 쿼드 코어 급으로 나오기 때문에 병렬 안에서 또 4개의 병렬이 생겨버린다. 이런 경우를 컨트롤하기 위해 만들어졌다.
```c
shared int n = ...;
shared double x[n], y[n] ;
private int i , my_first_element , my_last_element ;
my_first_element = ...;
my_last_element = ...;
/* Initialize x and y */
...
for ( i = my_first_element ; i <= my_last_element; i++)
     x[i] += y[i];
```


### Performance
> 병렬프로그래밍을 하는 이유는 뭐다? **성능** 이다!  

우리는 추가한 프로세스 개수와 비례하게 성능이 선형적으로 향상될 것이라고 기대한다. 하지만 실제로 그렇지는 않다.
왜냐하면 프로세스를 추가할 수록 오버헤드가 늘어나기 때문이다. 

이 이슈는 2가지로 설명할 수 있다.
- 프로세스를 추가할 수록 효율이 점점 떨어지더라(프로세스 간 오버헤드가 증가하므로).
- 문제의 스케일이 줄어들 수록 효율이 점점 떨어지더라. (바로 뒤에서 설명을 해줄 것이다) 
가 바로 그것이다.

#### 암달의 법칙
암달은 정확히는 성능의 최대치는 시리얼 섹션(Serial Section)의 크기를 넘어갈 수 없다는 것이 그 내용이다.
시리얼 섹션이란 프로그램 코드 중에서 병렬화 할 수 없는 부분을 의미하므로 어찌 보면 암달의 법칙은 매우 당연한 이야기다.

예를 들어 생각해보자. 90퍼센트의 코드만 병렬화를 시킬 수 있다면 시리얼 섹션은 그 중 10%가 될 것이다.
병렬화 한 쓰레드 개수가 \\(p\\), 병렬화 하지 않은 프로그램이 실행되는 데 걸리는 시간이 \\(T\\)고, 그 값이 \\(20\\)이라면 병렬화를 거칠 경우 그 프로그램이 실행되는 데 걸리는 시간은 
\\(0.9 * \frac{T}{p} + 0.1 * T = \frac{18}{p}+2\\)이다.  

이 때 성능향상(Speed up)은 다음과 같이 계산된다.

\$\$ S = \frac{T_{serial}}{0.9 * \frac{T_{serial}}{p} + 0.1 * T_{serial}} = \frac{20}{\frac{18}{p}+2} \$\$

그러나 암달은 문제 사이즈(problem size)를 고려하지 않았다. 앞 에서 문제 스케일(size)이 커지면 병렬 효율이 증가한다는 것을 생각하자.
위 식에서 문제 사이즈가 커지게 되면 \$ T_{serial} \$도 같이 증가해 분자가 커져버린다. 그래서 결국 상대적으로 더 좋은 효율을 가져갈 수 있다.


### Scalability
문제 스케일을 조절할 수 있다면 결국 성능 향상을 꾀할 수 있다. (스케일러블하다 = 사이즈를 조절할 수 있다)
- strongly scalable = 프로세스를 많이 투입하면 하는 대로 효율이 증가한다. (선형적으로)
- weakly scalable = 상대적으로 그렇지 못하다. (효율 감소가 아님, 점진적 증가)


### Taking Time, 시간 측정
퍼포먼스는 결국 병렬의 목적이라 했는데, 이를 측정하기 위해선 시간을 측정해야 한다.
언제 시작해서 언제 끝나느냐? 가 관건.
표준 C에서 제공하는 clock 함수는 사실 이상적인 런타임 시간은 아니다.

#### Wall Clock Time
Wall Clock은 장벽(start, finish) 2개 세워 놓고 그 사이 시간을 재는 것을 의미한다.
C의 clock은 milli초 단위로 세기 때문에 성능측정이 잘 되지 않는다.
```c
double start, finish;
...
start = Get_current_time();
/* 퍼포먼스를 측정할 코드 */
...
finish = Get_current_time();
printf("The elapsed tiem %e seconds", finish - start);
```

게다가 병렬 적으로 실행되므로 위의 코드는 사실 쓰레드(프로세스)마다 실행되는 것이다. 하지만 성능이라는 것은 각 프로세스의 성능을 평가하는 것이 아니라 병렬화된 전체 프로그램의 성능을 측정해야 하므로 위 방법은 별로 좋지 못하다. 게다가 이 경우 각자 프로세스는 시간을 재기 시작하는 시각이 제각각 틀리고 작업 마감 시간도 다 틀려지게 된다.

따라서 다음과 같은 방법을 쓴다. 
```c
shared double global_elapsed;
private double my_start, my_finish, my_elapsed;
...
/* 모든 쓰레드/프로세스 동기화 */
Barrier();
my_start = Get_current_time();
/* 퍼포먼스를 측정할 코드 */
...
my_finish = Get_current_time();
my_elapsed = my_finish - my_start;
/* 가장 늦게 끝나는 쓰레드/프로세스가 결국 성능을 결정한다 */
global_elapsed = Global_max(my_elapsed);
if (my_rank == 0) {
    printf("The elapsed tiem %e seconds", global_elapsed);
}
```

성능은 결국 `max(end[]) – min(start[])`를 재야 하는데 `min(start[])`끼리 차이가 많이 나버리면 성능이 구려진다. `min(start[])`를 동기화 시켜줘야(쓰레드/프로세스가 동시에 출발하도록) 정확한 성능측정이 된다는 것이다.
`Barrier()`는 바로 이 동기화에 쓰인다. 장벽에 모든 프로세스가 동작할 때 까지 잠깐 기다리세요 라는 의미이다. 이걸 거치게 되면 쓰레드/프로세스들의 start타임은 모두 똑같아 진다. 


### 병렬 프로그램 디자인
Foster의 방법론에서 병렬 프로그램은 결국 작업을 할당하고, 서로 커뮤니케이션 하고, 작업하고, 종국에는 결과를 종합해주는 작업을 하게 된다.

가령 일련의 데이터 배열에서 각각 구간별로 원소의 분포를 조사해야 하는 문제가 있다고 하자.
Serial Program에서는 for문 하나 돌면서 각 원소를 담는 새로운 공간에 하나씩 원소 count를 증가시키는 방식이 될 것이다.

이 작업을 4개로 분리하게 되면 배열을 4개로 쪼개 각 쓰레드/프로세스에 할당하고, 각 쓰레드/프로세스는 각각 원소를 담는 공간을 만들게 된다. 이후 카운트 하는 작업을 거친 뒤 나중에 하나로 합쳐주게 되는데, 합쳐줄 때는 본문 맨 처음 예시를 들었던 병렬화의 예시에서 처럼 진행한다. 


### 결국 무엇이 중요한가?
다음 4가지 이슈가 병렬 프로그램 디자인에서 중요한 요소이다.
- 속도 향상
- 효율
- 암달 법칙을 극복할 수 있는가?
- 문제의 스케일을 더 키울 수 있는가?

### MPI (Message Passing Interface)


읽어주셔서 감사합니다.



