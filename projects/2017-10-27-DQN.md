---
layout: post
title: Deep Q Network
category: [dqn, deep-learning]
author: hyungsun
image: assets/images/dqn_3.png
---

{% include image.html url="/images/dqn_1.jpg" description="이제는 전설이 되어버린 딥마인드의 벽돌깨기" source="https://www.youtube.com/watch?v=V1eYniJ0Rnk" %}
아래 글을 읽기 전에 동영상을 안보신 분은 이미지 아래 글귀를 클릭하여 한 번 보고 오시는 것을 추천드립니다.

### Deep Q Network는 어렵습니다!
정말 그렇습니다. 위 벽돌깨기 영상을 보고 인공지능의 매력에 흠뿍 빠져 방학동안 [딥마인드가 발표한 논문](http://files.davidqiu.com/research/nature14236.pdf)을 몇 번이나  읽었는데도 이해하는 것이 여간 쉽지 않았습니다. 물론 여기엔 아무런 바탕 지식이 없는 상태에서 헤딩한 것도 한 몫 했습니다.  

아무튼 제가 이해한 것을 바탕으로 포스팅을 하려 합니다. `DQN`을 이용한 프로젝트를 계획해두고 있어서 한 번 스스로도 정리하는 겸으로요. 

### Q-Learning
`DQN`을 알기 위해선 `Q-Leaning`에 대해 먼저 알고 있어야 합니다. 

아래 그림을 먼저 봅시다.
![]({{ site.url }}/assets/images/dqn_2.png)
위 그림은 인공지능이 어떠한 상태(`state`)에 직면했을 때 그것을 관측하여 어떤 행동을 하는 게 좋을지 (기억을 더듬어)정책 \\(\pi\\)를 세우고, 그 정책에 따라 행동 (`action`)을 취하게 된다는 것을 표현한 겁니다. 이 때 기억을 더듬는 과정에서 인공지능은 *'아, 이렇게 행동하면 이런 보상(`reward`)을 받았었지!'* 하며 가장 보상이 높은 행동을 선택합니다. 여기서 행동을 선택할 수 있게 해주는 녀석이 \\(\pi\\)라고 생각하시면 됩니다.

이 것은 마치 사람과 똑같습니다. 사람도 어떤 문제에 직면하게 되면, 잠시 생각할 시간을 갖고 어떻게 할 지 정합니다. 자신이 예전에 해결해 본 적이 있는 문제라면 그 기억을 참고해서 문제를 풀어나가겠죠. 결국 문제를 풀고 났을 때의 기쁨이 바로 인공지능에서의 보상(`reward`)과 같습니다.

여기서 여러분들이 떠오르게 될 첫번째 질문은 이겁니다.

> 인공지능이 얻는 보상(reward)은 어떻게 정하는데?

인공지능이 기쁨같은 걸 느낄리는 만무하죠. 그렇다고 엄마가 있는 것도 아니라서 잘했다고 사탕을 받지도 못합니다. 그래서 우리보다 똑똑한 사람들이 고민 끝에 생각해낸 아이디어가 바로  
  

**"자기 자신한테 잘하고 있냐고 물어본다"**  
  

입니다.   

#### 인공지능이 자아성찰을 한다고?
이 글을 읽고 계신 여러분은 '엣지 오브 투모로우' 라는 영화를 혹시 보셨을지 모르겠습니다. 개인적으로 정말 사랑하는 영화인데 앞으로 설명드릴 원리가 이 영화랑 완전히 똑같아서 이 영화에서 영감을 받아 탄생한 것이 `Q-Learning`일지도 모른다는 생각을 해봤습니다. (사실 `Q-Learning`이 먼저 등장했기 때문에 그럴 리는 없습니다.)

{% include image.html url="/images/81069d58663e777e82c067e9468d1e40.gif" description="엣지 오브 투모로우, 여러번 죽어보니 이젠 익숙하다" source="https://www.instiz.net/pt?no=3951382&page=1" %}

영화를 모르시는 분들을 위해 필요한 부분만 설명드리자면, 주인공인 톰 크루즈는 어쩌다 외계인이 침공한 전쟁터에서 죽어도 죽어도 되살아 나는 무한 루프에 빠지게 됩니다. 되살아 날 때는 이 전에 죽었던 기억들을 가지고 눈을 뜨게 되는데, 눈을 뜨게 되면 죽기 전과 똑같은 일상이 반복된다는 것을 깨닫게 됩니다. 톰 크루즈는 전에 죽었던 기억을 토대로 외계인의 재빠르고 위협적인 공격 패턴을 전부 외워서 모조리 회피하는 데 성공하고, 지구를 구하기 위해 앞장서게 됩니다.  

제가 앞서 이야기 했던 자아 성찰이라 함은 곧 영화에서의 '죽었던 기억'입니다. 앞에서 수백 수천번 죽고 나니 그 상황을 외우게 되어서, 결국에 어떤 행동을 해야 살아남을지 알게 된다는 것이죠. 이 때 오랫동안 살아 남는다는 것은 게임으로 치면 보상(점수)을 많이 받게되는 것을 의미하는데, 인공지능 역시 마찬가지입니다.

![]({{ site.url }}/assets/images/dqn_3.png)

여기서 \\(Q^\*\\)는 일종의 영혼 같은 녀석입니다. 이미 한 번 죽었던 나 자신이죠. 그 영혼이 말해줍니다. *'내가 해봐서 아는데, 상태 `s`에서 행동`a`를 하면 이 만큼의 보상`r`을 얻어.'* 라고 말이죠. 즉 `s`와 `a`를 주면 `r`을 알려준다 이 말입니다. 
그런데 보상을 왜 다 더했을까요? 만약 보상을 다 더하지 않고 그때 그때 단발적으로 얻는 보상만 \\(Q^\*\\)에 저장했다면 다음과 같은 문제가 생겼을 겁니다.

- A 루트: 처음 보상은 적지만 승리할 가능성이 있음, 보상은 각각 +1, +10, 총 합: 11
- B 루트: 시작부터 큰 보상을 받지만 무조건 패배, 보상은 각각 +5, -1, 총 합: 4

이런 루트가 있는데 학습을 시킨다면 인공지능은 무조건 B루트만 선택하고 패배합니다. 인공지능한테 보이는 것은 최종 보상인 11 과 4 가 아닌 처음 시작 보상인 1 과 5 밖에 보이지 않을 테니까요. 결국 다 더해주지 않으면 장기적인 보상을 계산할 수 없게 되는 겁니다. 최적의 행동을 취하기 위해서는 장기적인 보상이 최대일 때를 생각해서 결정해야 하는 데도 불구하고요.

그렇다면 \\({\gamma}\\) 는 뭘까요? \\({\gamma}\\) 는 보통 \\(0<{\gamma}<1\\) 의 값을 가지는데, 수학적으로 생각해보면 나중에 얻는 보상이 점점 작아진다는 느낌이 있죠. 1보다 작은 수를 계속 곱했으니까요. 그렇게 이해했다면 잘 이해하신 겁니다. 이해를 돕기 위해 이번에도 인공지능이 아래 2가지를 선택한다고 해봅시다.

- 골고루 벽돌을 부순다.
- 왼쪽 벽돌만 후벼 판다.

왼쪽만 후벼파게 되면 결국 천장을 뚫게 되어 엄청난 점수를 획득할 수 있는 기회를 얻게 되지만(상단 동영상 링크 참조), 골고루 벽돌을 부수게 되면 그런 기회는 얻기가 힘들게 됩니다. 하지만 이미 골고루 벽돌을 부순다를 선택했다면 다시 번복할 수는 없죠. 이미 벽돌은 골고루 깨어져 있기 때문입니다. 그래서 처음 내린 결정이 장기적인 보상을 결정하는 데 더 큰 영향을 미친다고 판단하여, 현재 얻는 보상이 미래에 얻는 보상에 비해 얼마나 더 중요한지를 표현한 값이 바로 \\({\gamma}\\) 입니다.

여기서 여러분이 할 만한 두 번째 질문은 이겁니다.
> \\(Q^\*\\) 를 도대체 어디다 저장하는데?

톰 크루즈가 머릿속에서 기억을 떠올리듯이, 인공지능 역시 메모리 공간에서 \\(Q^\*\\) 를 가져옵니다. 이 공간을 `Q-table` 이라고 하는데 이는 \\(Q^\*\\) 를 저장하는 가장 기본적인 방법입니다. 앞에서 설명했다시피 \\(Q^\*\\) 은 이미 여러 번 죽어봐서 어떤 행동을 했을 때 어떤 보상을 알지 전부 알고 있습니다. 그걸 표현한 것이 다음 그림입니다.

![]({{ site.url }}/assets/images/dqn_5.png)

현재 상태는 \\(S_{2}\\) 인데 가장 최적의 선택은 \\(a_{3}\\) 라고 \\(Q^\*\\) 가 알려주고 있죠. 언뜻 보기엔 \\(Q^\*\\) 를 저장하는 이 방법은 굉장히 직관적이고 합리적으로 보입니다. 하지만 스타크래프트 같이 고려할 변수가 엄청나게 많은 데다 쉽사리 끝나지도 않는 게임을 학습하게 된다면 어떨까요? 메모리가 버티다 못해 터져버리고 말겁니다.

### Deep Leaning
`Q-table`이 비효율적이라는 것을 깨달은 사람들은 `Neural Network`로 눈을 돌렸습니다. `s`, `a` 에 대한 `r`의 값을 굳이 다 테이블에 저장하지 않고, `s`와 `a`를 집어넣으면 `r`이 나오게끔 하는 `Neural Network`를 만들어 버리자는게 아이디어였죠. 이런 `Neural Network` 중에서도 층을 겹겹이 쌓아 만든 녀석을 `Deep Neural Network`라고 부르고, 아래 그림과 같이 생겼습니다.

![]({{ site.url }}/assets/images/dqn_6.png)

`Deep Neural Network`을 이용해 학습하는 모델을 `Deep Learning`이라고 부릅니다.
이 부분은 사실 `Neural Network`에 대한 지식이 없으면 이해하기가 힘든 부분인데, 관련한 내용은 나중에 포스팅하여 링크를 걸도록 하겠습니다. 우리보다 똑똑한 사람들이 해놓은 거니 이해하지 않고 그냥 받아들이는 것도 당장 나쁜 선택은 아닙니다. 

기억해두어야 할 사실은, 테이블을 사용하지 않고도 `s`와 `a`를 이용해 `r`을 얻을 수 있는 무기가 저희에게 생겼다는 겁니다.

### Deep Q Network

대망의 `DQN`으로 접어들었습니다. 하지만 아직 갈 길이 멀죠. `DQN`은 실제로 딥마인드에서 썼던 알고리즘이니 논문을 참고해서 설명드리겠습니다.

#### 전처리(preprocessing)
일단 딥마인드에서는 `Deep Neural Network`에 `input`으로 집어넣기 전에 이미지 전처리(`preprocessing`) 과정을 거쳤습니다. 
![]({{ site.url }}/assets/images/dqn_7.png)
쉽게 말해 좀 더 네트워크에 쓰기 알맞게 데이터를 가공한 겁니다. 

만약 이미지가 한 장만 있다면 정지된 것처럼 보이므로 공이 어디로 튈지 모르게 됩니다. 딥마인드는 연속된 `m`개의 이미지를 쌓아올려 `Q` 함수에 쓸 `input`을 준비했습니다.
![]({{ site.url }}/assets/images/dqn_8.png)

이미지를 왼쪽부터 차례대로 자세히 보시면 공이 점점 아래로 내려오는 걸 확인할 수가 있습니다. 즉 연속된 이미지를 넣어준다는 것은 공이 움직이는 상태(`state`)를 넣겠다는 것이고, 앞에서 이야기 했던 `s`가 바로 여기에 해당됩니다.

#### 모델 최적화 (Model Architecture Optimization)
`Q`에 `s`를 넣었으니 여태 이야기했던 것처럼 `a`를 마저 넣어서 얼른 `r`를 구하고 싶으실 겁니다. 딥마인드도 처음엔 이 방식대로 했다가 금방 접었습니다. 그 이유는
`DQN`이 `input`으로 `s`와 `a`를 받는 다면 행동의 수만큼 `DQN`을 더 거쳐야하므로 비용이 상당해지기 때문이죠.  

그래서 딥마인드는 모델을 조금 변형했습니다.  
![]({{ site.url }}/assets/images/dqn_9.png)  
`input`에서 `a`를 빼버리고, 대신 `Q-value`를 여러개 뽑아내서 그 중에 최대값인 `Q-value`에 해당하는 `a`를 선택하자는게 이 모델의 핵심 아이디어입니다. `Neural Network`라는 것은 굳이 `output`을 하나로 정할 필요가 없습니다. 심지어 어떻게 디자인 하느냐에 따라 여러 개의 `output`을 뽑아낼 수도 있죠. 이 점을 이용해 `Neural Network`가 뿜어내는 `output`을 각각 `a`에 매칭시키고 그 중 `Q-value`가 최대인 `a`를 선택합니다. 결과는 동영상에서 확인했다시피 상당히 성공적입니다.

#### 합성곱 신경망(CNN, Convolutional Neural Network)
`Deep Neural Network`의 종류중 하나로 `CNN(Convolutional Neural Network)`이라는 것이 있습니다. 사실 `CNN`하나만 설명해도 분량이 엄청나게 많아지기 때문에 간략히만 설명을 적어놓겠습니다.   
`CNN`을 나타내고 있는 아래 그림을 먼저 봅시다.
![]({{ site.url }}/assets/images/dqn_10.png)
오른쪽 결과물에 `dog(0.01)`, `cat(0.04)`, `boat(0.94)`, `bird(0.02)`라는 게 보이시나요? 이 `output`들은 각각 그 카테고리에 해당할 확률을 나타냅니다. 이 이미지가 `dog`에 해당할 확률은 고작 1% 인 반편 `boat`에 해당할 확률은 무려 94% 이므로 이 `CNN`은 "해당 이미지가 보트 이미지다" 라고 말하고 있는 겁니다. 

`CNN`은 이렇게 2차원 이미지 처리에서 엄청난 성능을 보인다는 특징이 있습니다.  

갑자기 이 이야기를 하는 이유는 딥마인드에서 `Q`를 저장할 용도로 이 `CNN`을 사용했기 때문입니다. `input`이 앞서 전처리를 거친 4개의 이미지이고 `output`이 바로 `Q-value`가 되는 것이죠. 

![]({{ site.url }}/assets/images/dqn_11.png)

정확히는 위와 같은 과정을 거쳤습니다. `Convolutional Layer` 3개와 `Fully Connected Layer`2개를 이어 붙여서 거대한 `CNN`을 만든 것이죠. `input`이 84 x 84 x 4 인 것은 전처리 과정에서 이미지 하나 당 84 x 84 사이즈로 리사이징을 했기 때문이고, 맨 마지막 `Output`이 6인 것은 벽돌깨기에서 취할 수 있는 행동이 총 6가지이기 때문입니다.

#### Q 최적화(Optimizer)
사실 여기까지 오면서 이해가 가지 않았던 부분이 하나 있었을 겁니다.

> 도대체 Q는 어떻게 업데이트 하는거야?

인공지능을 조금이라도 공부했다면 `gradient descent`를 들어보셨을 겁니다. `gradient descent`를 이용하면 가중치 `theta`를 업데이트 할 수가 있었죠. `Deep Neural Network`에도 여전히 가중치라는 것이 존재합니다. 따라서 비슷한 방식으로 업데이트 역시 할 수 있습니다. 이 것 역시 당장은 이해가 가지 않아도 상관 없습니다.  

딥마인드에서는 `Optimizer`로 `RMSProp` 을 사용했습니다. 그냥 `gradient descent`의 후손이라고 생각하시면 됩니다. 대부분의 경우에서 성능이 더 좋은 녀석이죠. `gradient descent`를 작동시키려면 각 `data set` 마다의 비용(`cost`)이 필요했듯이 `RMSProp` 또한 마찬가지 입니다. (논문에서는 `Loss`로 나옵니다)

#### 탐험률(Exploration Rate)
엣지 오브 투모로우에서 톰 크루즈가 외계인을 아무리 학살해봤자 가능성이 없다는 것을 깨닫고 자포자기 한 상태로 파리로 도망가는 장면이 있습니다. 파리가 맞았나요? 정확히 기억은 안나네요. 아무튼 그 곳에서 외계인과의 전쟁을 승리로 이끌 단서를 발견하게 되죠. 만약 주인공이 정말 끈질겨서 절대 포기하지 않는 성격이었다면 외계인과의 전쟁은 영영 패배했을지도 모르는 일입니다.  

학습과는 전혀 연관성이 없는 이러한 '운' 의 가능성을 염두에 둔 것이 바로 탐험률입니다. 인공지능으로 하여금 가끔씩 이상한 선택을 하게 하여 그 '운'을 노려보는 것이죠. 기호로는 앱실론(\\({\epsilon}\\)) 으로 표현됩니다.  

딥마인드의 벽돌깨기 역시 탐험률을 적용시켰습니다.

#### 비용 함수(Loss Function)
`DQN`에서는 앞서 이야기 했던 `CNN`을 2개 사용합니다. 그것도 똑같은 모양으로요. 각각 \\(Q^\*\\) 와 \\(Q\\) 를 저장하기 위함입니다.  

딥마인드에서는 10000 스탭 정도 지날 때마다 \\(Q^\*\\) 의 가중치에다가 \\(Q\\) 의 가중치를 덮어 씌움으로써 계속 경험을 쌓아 나갑니다. `cost`를 구하기 위해서는 실제 값에다가 예상한 값(기대값)을 빼서 제곱을 하는 방식을 많이 사용합니다. 이 때 실제값을 구하기 위해 \\(Q^\*\\)을 이용하고, 기대값을 구하기 위해 \\(Q\\)를 사용합니다. 여기서 구한 `cost`로 \\(Q\\)를 업데이트합니다. 비용을 구하는 방법은 아래에 나와 있습니다.

![]({{ site.url }}/assets/images/dqn_12.png)

> 왜 굳이 CNN을 2개 사용하나?

위 공식에서 `target`은 결국 실제 값을 의미하는데, 이 값이 `network` 가중치에 의존하기 때문에 생기는 문제를 방지하기 위해 네트워크를 분리한 `target network`를 사용합니다.

#### Experiance Replay
학습이 진행되면서 얻은 새로운 경험을 곧바로 학습하지 않고 `experience replay`를 넣는 공간(`D`)에 저장한 후 학습할 때 `replay`를 이용하여 학습합니다. `replay`를 이용했다는 것은 `random sampling` 한 `mini-batch`를 구성해 학습했다는 것을 의미합니다. 즉 경험 데이터의 순서를 무작위로 하여 데이터들간의 상호관계를 깬 것인데, 이렇게 되면 귀하고 중요한 경험을 여러번 학습할 수 있는 기회를 얻게 됩니다. 

데이터들간의 상호관계를 깼다는 말은 무작위로 `mini-batch`를 했으니 당연한 것이고, 매 `step`마다 `experience replay`를 진행하니 중요한 경험들을 몇번이고 꺼내와서 학습할 수 있는 기회를 얻게 되는 겁니다.


### Deep Q-learning with experiance replay
앞서 이야기 했던 내용을 슈도코드로 표현하면 아래와 같습니다.  

![]({{ site.url }}/assets/images/dqn_13.png)


### 끝으로
이해하는 것보다 글로 쓰는 게 더 힘드네요.. 읽느라 고생하셨습니다.
